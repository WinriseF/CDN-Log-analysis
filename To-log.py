import os
import gzip
from collections import Counter
import re

MERGED_LOG = "merged_logs.log"
BLACKLIST_IP = "blacklist_ip.txt"
TOP_IP = "top_ip.txt"
SPIDER_LOG = "spiders.log"

# å¸¸è§æ‰«æå™¨å…³é”®å­—
SPIDER_KEYWORDS = [
    "CensysInspect",
    "curl",
    "Wget",
    "bot",
    "crawl",
    "scanner",
    "security",
    "nmap"
]

# æ—¥å¿—æ­£åˆ™ï¼Œå’Œå‰é¢åˆ†æè„šæœ¬ä¸€è‡´
LOG_PATTERN = re.compile(
    r'\[(?P<time>.*?)\]\s+'
    r'(?P<ip>\S+)\s+'
    r'(?P<resp_time>\d+)\s+'
    r'"(?P<referer>.*?)"\s+'
    r'"(?P<protocol>.*?)"\s+'
    r'"(?P<method>.*?)"\s+'
    r'"(?P<domain>.*?)"\s+'
    r'"(?P<path>.*?)"\s+'
    r'(?P<status>\d+)\s+'
    r'(?P<size>\d+)\s+'
    r'(?P<hit>\S+)\s+'
    r'"(?P<ua>.*?)"\s+'
    r'"(?P<other>.*?)"\s+'
    r'(?P<source_ip>\S+)'
)

def parse_log_line(line):
    m = LOG_PATTERN.match(line)
    return m.groupdict() if m else None

def merge_gz_files():
    gz_files = sorted(f for f in os.listdir('.') if f.endswith('.gz'))

    if not gz_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• .gz æ–‡ä»¶")
        return

    print(f"ğŸ” æ‰¾åˆ° {len(gz_files)} ä¸ª .gz æ–‡ä»¶ï¼Œå¼€å§‹åˆå¹¶â€¦")

    with open(MERGED_LOG, 'wb') as out_f:
        for gz_file in gz_files:
            print(f"â¡ åˆå¹¶ï¼š{gz_file}")
            with gzip.open(gz_file, 'rb') as f:
                for chunk in iter(lambda: f.read(1024 * 1024), b''):
                    out_f.write(chunk)

    print(f"ğŸ“¦ åˆå¹¶å®Œæˆï¼š{MERGED_LOG}\n")

def detect_spider(ua):
    if not ua:
        return False
    ua_lower = ua.lower()
    return any(key.lower() in ua_lower for key in SPIDER_KEYWORDS)

def analyze_log():
    if not os.path.exists(MERGED_LOG):
        print("âŒ merged_logs.log ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆå¹¶æ—¥å¿—ï¼")
        return

    print("ğŸ“Š å¼€å§‹åˆ†ææ—¥å¿—â€¦")

    ip_counter = Counter()
    spider_entries = []

    with open(MERGED_LOG, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            data = parse_log_line(line)
            if not data:
                continue

            ip_counter[data['ip']] += 1

            if detect_spider(data['ua']):
                spider_entries.append(line)

    # è¾“å‡º TOP 100 IP
    with open(TOP_IP, 'w', encoding='utf-8') as f:
        for ip, count in ip_counter.most_common(100):
            f.write(f"{ip} {count}\n")

    # è¾“å‡º spider æ—¥å¿—
    with open(SPIDER_LOG, 'w', encoding='utf-8') as f:
        f.writelines(spider_entries)

    # ç”Ÿæˆé»‘åå•ï¼ˆè®¿é—® 100 æ¬¡ä»¥ä¸Š æˆ– å±äºæ‰«æå™¨ï¼‰
    blacklist = set(ip for ip, count in ip_counter.items() if count > 100)
    blacklist.update(data['ip'] for line in spider_entries if (data := parse_log_line(line)))

    with open(BLACKLIST_IP, 'w', encoding='utf-8') as f:
        for ip in blacklist:
            if ip:
                f.write(ip + "\n")

    print(f"""
ğŸ‰ åˆ†æå®Œæˆï¼

ğŸ“Œ è®¿é—®é‡ TOP 100 IPï¼š
    {TOP_IP}

ğŸ“Œ çˆ¬è™« & æ‰«æå™¨æ—¥å¿—ï¼š
    {SPIDER_LOG}

ğŸ“Œ è‡ªåŠ¨ç”Ÿæˆé»‘åå•ï¼š
    {BLACKLIST_IP}

âœ” æ”¯æŒ Censys / curl / Wget / nmap / å„ç±»æ‰«æå™¨è‡ªåŠ¨è¯†åˆ«
âœ” æ”¯æŒå¤§æ—¥å¿—ï¼ˆæµå¼å¤„ç†ï¼‰
""")

if __name__ == "__main__":
    merge_gz_files()
    analyze_log()
